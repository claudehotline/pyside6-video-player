{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.model import parsingNet\n",
    "from torch.onnx import export\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cv2\n",
    "from data.constant import culane_row_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\anaconda3\\envs\\deeplane\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "i:\\anaconda3\\envs\\deeplane\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "net = parsingNet(pretrained = False, backbone='18',cls_dim = (201,18,4),use_aux=False) # we dont need auxiliary segmentation in testing\n",
    "\n",
    "state_dict = torch.load('checkpoints/culane_18.pth', map_location='cpu')['model']\n",
    "compatible_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "  if 'module.' in k:\n",
    "    compatible_state_dict[k[7:]] = v\n",
    "  else:\n",
    "    compatible_state_dict[k] = v\n",
    "\n",
    "net.load_state_dict(compatible_state_dict, strict=False)\n",
    "# print(net)\n",
    "# 设置模型为推理模式\n",
    "net.eval()\n",
    "\n",
    "x = torch.randn(1, 3, 288, 800)\n",
    "torch_out = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(net, x, 'culane_18.onnx', verbose=True, input_names=['input'], output_names=['output'], opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load('culane_18.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "# onnx_model = onnx.load(\"test.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"culane_18.onnx\")\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 288, 800])\n",
      "(201, 18, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img_path = 'I:/Ultra-Fast-Lane-Detection/CULane/driver_37_30frame/05181432_0203.MP4/04320.jpg'\n",
    "\n",
    "img = Image.open(img_path)\n",
    "# print(type(img))\n",
    "# image = np.asarray(img, dtype=np.float32)\n",
    "# image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "img_transforms = transforms.Compose([\n",
    "        transforms.Resize((288, 800)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "\n",
    "image = img_transforms(img).unsqueeze(0)\n",
    "print(image.shape)\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(image)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "ort_outs = ort_outs[0][0]\n",
    "# print(ort_outs)\n",
    "\n",
    "print(ort_outs.shape)\n",
    "\n",
    "img_w, img_h = 1640, 590\n",
    "row_anchor = culane_row_anchor\n",
    "cls_num_per_lane = 18\n",
    "\n",
    "col_sample = np.linspace(0, 800 - 1, 200)\n",
    "col_sample_w = col_sample[1] - col_sample[0]\n",
    "\n",
    "ort_outs = ort_outs[:, ::-1, :]\n",
    "prob = scipy.special.softmax(ort_outs[:-1, :, :], axis=0)\n",
    "idx = np.arange(200) + 1\n",
    "idx = idx.reshape(-1, 1, 1)\n",
    "loc = np.sum(prob * idx, axis=0)\n",
    "out_j = np.argmax(ort_outs, axis=0)\n",
    "loc[out_j == 200] = 0\n",
    "out_j = loc\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "vis = cv2.imread(img_path)\n",
    "for i in range(out_j.shape[1]):\n",
    "    if np.sum(out_j[:, i] != 0) > 2:\n",
    "        for k in range(out_j.shape[0]):\n",
    "            if out_j[k, i] > 0:\n",
    "                ppp = (int(out_j[k, i] * col_sample_w * img_w / 800) - 1, int(img_h * (row_anchor[cls_num_per_lane-1-k]/288)) - 1 )\n",
    "                cv2.circle(vis,ppp,5,(0,255,0),-1)\n",
    "cv2.imwrite('result.jpg', vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplane",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
